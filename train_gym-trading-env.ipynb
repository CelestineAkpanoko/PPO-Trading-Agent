{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fea16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium gym_trading_env tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0009478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ppo_trading_agent import PPO\n",
    "from utils import plot_performance\n",
    "import os\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffb7fdb0",
   "metadata": {},
   "source": [
    "- Data download\n",
    "- BTC/USDT historical data from Binance and stores it to directory ./data/binance-BTCUSDT-1h.pkl\n",
    "from gym_trading_env.downloader import download \n",
    "download(exchange_names = [\"binance\"],\n",
    "     symbols= [\"BTC/USDT\"],\n",
    "     timeframe= \"1h\",\n",
    "     dir = \"data\",\n",
    "     since= datetime.datetime(year= 2020, month= 1, day=1),\n",
    " )\n",
    " \n",
    " A VPN is needed to download the data from Binance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a033571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "df = pd.read_pickle(\"./data/binance-BTCUSDT-1h.pkl\")\n",
    "df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "df[\"feature_open\"] = df[\"open\"] / df[\"close\"]\n",
    "df[\"feature_high\"] = df[\"high\"] / df[\"close\"]\n",
    "df[\"feature_low\"] = df[\"low\"] / df[\"close\"]\n",
    "df[\"feature_volume\"] = df[\"volume\"] / df[\"volume\"].rolling(7 * 24).max()\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make(\"TradingEnv\",\n",
    "               name=\"BTCUSDT\",\n",
    "               df=df,  # Your dataset with your custom features\n",
    "               positions=[-1, 0, 1],  # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "               trading_fees=0.01/100,  # 0.01% per stock buy / sell\n",
    "               borrow_interest_rate=0.0003/100)  # 0.0003% per timestep\n",
    "\n",
    "env.unwrapped.add_metric('Position Changes', lambda history: np.sum(np.diff(history['position']) != 0))\n",
    "env.unwrapped.add_metric('Episode Length', lambda history: len(history['position']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ddc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Agent setup\n",
    "n_episodes = 200\n",
    "n_actions = env.action_space.n\n",
    "input_dims = env.observation_space.shape[0]\n",
    "alpha = 0.0003\n",
    "gamma = 0.99\n",
    "policy_clip = 0.2\n",
    "n_epochs = 4\n",
    "gae_lambda = 0.95\n",
    "batch_size = 64\n",
    "chkpt_dir = './models/'\n",
    "figure_file = './plots/performance.png'\n",
    "\n",
    "if not os.path.exists(chkpt_dir):\n",
    "    os.makedirs(chkpt_dir)\n",
    "if not os.path.exists(os.path.dirname(figure_file)):\n",
    "    os.makedirs(os.path.dirname(figure_file))\n",
    "\n",
    "ppo_agent = PPO(n_actions, input_dims, alpha, gamma, policy_clip, n_epochs, gae_lambda, batch_size, chkpt_dir)\n",
    "\n",
    "# Define thresholds for stopping criteria\n",
    "portfolio_return_threshold = 0.95  # 95% portfolio return\n",
    "portfolio_returns = []\n",
    "# Initialize lists to store metrics\n",
    "rewards = []\n",
    "total_losses = []\n",
    "portfolio_returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(n_episodes)):\n",
    "    score = 0\n",
    "    done, truncated = False, False\n",
    "    observation, info = env.reset()\n",
    "    while not done and not truncated:\n",
    "        action, log_prob, value = ppo_agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        ppo_agent.store_transition(observation, action, log_prob, value, reward, done)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "\n",
    "    rewards.append(score)\n",
    "    # Retrieve and store the \"Portfolio Return\" metric\n",
    "    metrics = env.unwrapped.get_metrics()\n",
    "    portfolio_return = metrics.get('Portfolio Return', None)\n",
    "    if portfolio_return is not None:\n",
    "        portfolio_returns.append(portfolio_return)\n",
    "    else:\n",
    "        print(f\"Portfolio Return not found for episode {i}\")\n",
    "\n",
    "    # Convert to numpy array\n",
    "    portfolio_returns_np = np.array([float(val.strip('%')) for val in portfolio_returns])\n",
    "    actor_loss, critic_loss = ppo_agent.learn()\n",
    "    total_loss = actor_loss + critic_loss\n",
    "    total_losses.append(total_loss)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Print metrics every 10 episodes\n",
    "    if (i + 1) % 10 == 0:\n",
    "        avg_total_loss = np.mean(total_losses[max(0, i-9):i+1])  # Average total loss over the last 10 episodes\n",
    "        print(f'Episode {i+1}, Score: {score}, Portfolio Return: {portfolio_returns[-1]}, Avg Total Loss: {avg_total_loss:.4f}')\n",
    "\n",
    "    portfolio_returns_np = np.array([float(val.strip('%')) for val in portfolio_returns])\n",
    "    # Check stopping criteria over the last 50 episodes\n",
    "    if i >= 50:\n",
    "        avg_portfolio_return = np.mean(portfolio_returns_np[-20:])\n",
    "        if avg_portfolio_return > portfolio_return_threshold:\n",
    "            print(\"Stopping criteria met: Market Return and Portfolio Return thresholds reached.\")\n",
    "            break\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate total training time\n",
    "total_training_time = end_time - start_time\n",
    "print(f\"Total training time: {total_training_time:.2f} seconds\")\n",
    "# Assuming portfolio_returns is a list of strings with percent symbols\n",
    "portfolio_returns_np = np.array([float(val.strip('%')) for val in portfolio_returns])\n",
    "np.save('portfolio_returns.npy', portfolio_returns_np)\n",
    "# Save rewards and losses as numpy files\n",
    "np.save('rewards.npy', np.array(rewards))\n",
    "np.save('total_losses.npy', np.array(total_losses))\n",
    "ppo_agent.save_models()\n",
    "x = [i+1 for i in range(len(rewards))]\n",
    "plot_performance(x, rewards, total_losses, portfolio_returns_np, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106deab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
